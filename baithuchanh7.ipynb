{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e058c169-b2d0-4190-9585-f99384ff8f92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbfs:/FileStore/shared_uploads/danghia2002@gmail.com/Churn_Modelling.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1b00ea-7412-428a-a70d-b18160a3ec48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Sử dụng PySpark để đọc dữ liệu vào DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "#Khởi tạo SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "#Đọc dữ liệu từ file CSV\n",
    "data = spark.read.csv(\"dbfs:/FileStore/shared_uploads/danghia2002@gmail.com/Churn_Modelling.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58ec39c-a98f-4221-be88-e47d5c908b05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- RowNumber: integer (nullable = true)\n |-- CustomerId: integer (nullable = true)\n |-- Surname: string (nullable = true)\n |-- CreditScore: integer (nullable = true)\n |-- Geography: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- Tenure: integer (nullable = true)\n |-- Balance: double (nullable = true)\n |-- NumOfProducts: integer (nullable = true)\n |-- HasCrCard: integer (nullable = true)\n |-- IsActiveMember: integer (nullable = true)\n |-- EstimatedSalary: double (nullable = true)\n |-- Exited: integer (nullable = true)\n\nNumber of rows:  10000\nNumber of columns:  14\n+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n|RowNumber|CustomerId|  Surname|CreditScore|Geography|Gender|Age|Tenure|  Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n|        1|  15634602| Hargrave|        619|   France|Female| 42|     2|      0.0|            1|        1|             1|      101348.88|     1|\n|        2|  15647311|     Hill|        608|    Spain|Female| 41|     1| 83807.86|            1|        0|             1|      112542.58|     0|\n|        3|  15619304|     Onio|        502|   France|Female| 42|     8| 159660.8|            3|        1|             0|      113931.57|     1|\n|        4|  15701354|     Boni|        699|   France|Female| 39|     1|      0.0|            2|        0|             0|       93826.63|     0|\n|        5|  15737888| Mitchell|        850|    Spain|Female| 43|     2|125510.82|            1|        1|             1|        79084.1|     0|\n|        6|  15574012|      Chu|        645|    Spain|  Male| 44|     8|113755.78|            2|        1|             0|      149756.71|     1|\n|        7|  15592531| Bartlett|        822|   France|  Male| 50|     7|      0.0|            2|        1|             1|        10062.8|     0|\n|        8|  15656148|   Obinna|        376|  Germany|Female| 29|     4|115046.74|            4|        1|             0|      119346.88|     1|\n|        9|  15792365|       He|        501|   France|  Male| 44|     4|142051.07|            2|        0|             1|        74940.5|     0|\n|       10|  15592389|       H?|        684|   France|  Male| 27|     2|134603.88|            1|        1|             1|       71725.73|     0|\n|       11|  15767821|   Bearce|        528|   France|  Male| 31|     6|102016.72|            2|        0|             0|       80181.12|     0|\n|       12|  15737173|  Andrews|        497|    Spain|  Male| 24|     3|      0.0|            2|        1|             0|       76390.01|     0|\n|       13|  15632264|      Kay|        476|   France|Female| 34|    10|      0.0|            2|        1|             0|       26260.98|     0|\n|       14|  15691483|     Chin|        549|   France|Female| 25|     5|      0.0|            2|        0|             0|      190857.79|     0|\n|       15|  15600882|    Scott|        635|    Spain|Female| 35|     7|      0.0|            2|        1|             1|       65951.65|     0|\n|       16|  15643966|  Goforth|        616|  Germany|  Male| 45|     3|143129.41|            2|        0|             1|       64327.26|     0|\n|       17|  15737452|    Romeo|        653|  Germany|  Male| 58|     1|132602.88|            1|        1|             0|        5097.67|     1|\n|       18|  15788218|Henderson|        549|    Spain|Female| 24|     9|      0.0|            2|        1|             1|       14406.41|     0|\n|       19|  15661507|  Muldrow|        587|    Spain|  Male| 45|     6|      0.0|            1|        0|             0|      158684.81|     0|\n|       20|  15568982|      Hao|        726|   France|Female| 24|     6|      0.0|            2|        1|             1|       54724.03|     0|\n+---------+----------+---------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\nonly showing top 20 rows\n\n+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n|summary|         RowNumber|       CustomerId|Surname|      CreditScore|Geography|Gender|               Age|            Tenure|          Balance|     NumOfProducts|          HasCrCard|     IsActiveMember|  EstimatedSalary|             Exited|\n+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n|  count|             10000|            10000|  10000|            10000|    10000| 10000|             10000|             10000|            10000|             10000|              10000|              10000|            10000|              10000|\n|   mean|            5000.5|  1.56909405694E7|   null|         650.5288|     null|  null|           38.9218|            5.0128|76485.88928799961|            1.5302|             0.7055|             0.5151|100090.2398809998|             0.2037|\n| stddev|2886.8956799071675|71936.18612274907|   null|96.65329873613035|     null|  null|10.487806451704587|2.8921743770496837|62397.40520238599|0.5816543579989917|0.45584046447513327|0.49979692845891815|57510.49281769821|0.40276858399486065|\n|    min|                 1|         15565701|  Abazu|              350|   France|Female|                18|                 0|              0.0|                 1|                  0|                  0|            11.58|                  0|\n|    max|             10000|         15815690| Zuyeva|              850|    Spain|  Male|                92|                10|        250898.09|                 4|                  1|                  1|        199992.48|                  1|\n+-------+------------------+-----------------+-------+-----------------+---------+------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-----------------+-------------------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n"
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "data:image/png;base64,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\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "image"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Thực hiện một số thống kê, trực quan hóa để hiểu dữ liệu\n",
    "#Hiển thị thông tin cấu trúc của DataFrame\n",
    "data.printSchema()\n",
    "#Hiển thị số lượng dòng và cột trong DataFrame\n",
    "print(\"Number of rows: \", data.count())\n",
    "print(\"Number of columns: \", len(data.columns))\n",
    "#Hiển thị một số dòng đầu tiên trong DataFrame\n",
    "data.show()\n",
    "#Thực hiện các thống kê mô tả tổng quan của DataFrame\n",
    "data.describe().show()\n",
    "#Trực quan hóa dữ liệu bằng biểu đồ, ví dụ: histogram của cột Age\n",
    "import matplotlib.pyplot as plt\n",
    "age_values = data.select(\"Age\").rdd.flatMap(lambda x: x).collect()\n",
    "plt.hist(age_values, bins=20)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3d201f-37e0-4cfd-b05a-70847ec1d761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Tiền xử lý dữ liệu\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#Loại bỏ cột RowNumber, CustomerId\n",
    "data = data.drop(\"RowNumber\", \"CustomerId\")\n",
    "#Chuyển đổi giá trị chuỗi thành số\n",
    "string_columns = [\"Geography\", \"Gender\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(data) for column in string_columns]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "data = pipeline.fit(data).transform(data)\n",
    "#Chuyển đổi các biến độc lập thành vector\n",
    "feature_columns = [\"CreditScore\", \"Geography_index\", \"Gender_index\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ad637a-af76-45c9-9fe7-1e2ee117883a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Chia dữ liệu thành tập huấn luyện và tập kiểm tra(tỉ lệ 70/)\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4877f314-527d-436f-a2c3-67420c7e938d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Sử dụng Logistic Regression để huấn luyện mô hình\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "#Khởi tạo mô hình Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Exited\")\n",
    "#Huấn luyện mô hình trên tập huấn luyện\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a24c224-8735-403c-bf63-a45b11e24fa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8137638617090672\n"
     ]
    }
   ],
   "source": [
    "#Đánh giá hiệu suất của mô hình trên tập kiểm tra\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "#Tạo một evaluator với metricName='accuracy'\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Exited\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "#Tính toán accuracy trên tập kiểm tra\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a696ed4-fde7-4bf3-aad8-5847e8eade44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2639336321475893>:5\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline(stages\u001B[38;5;241m=\u001B[39mindexers \u001B[38;5;241m+\u001B[39m [assembler] \u001B[38;5;241m+\u001B[39m [lr])\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#Huấn luyện mô hình từ dữ liệu huấn luyện\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mfit(train_data)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    210\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:132\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(stage, Transformer):\n",
       "\u001B[1;32m    131\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(stage)\n",
       "\u001B[0;32m--> 132\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mstage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n",
       "\u001B[1;32m    134\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n",
       "\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:400\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n",
       "\u001B[0;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, dataset\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Output column Geography_index already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-2639336321475893>:5\u001B[0m\n\u001B[1;32m      3\u001B[0m pipeline \u001B[38;5;241m=\u001B[39m Pipeline(stages\u001B[38;5;241m=\u001B[39mindexers \u001B[38;5;241m+\u001B[39m [assembler] \u001B[38;5;241m+\u001B[39m [lr])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#Huấn luyện mô hình từ dữ liệu huấn luyện\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mfit(train_data)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:132\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(stage, Transformer):\n\u001B[1;32m    131\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(stage)\n\u001B[0;32m--> 132\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mstage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n\u001B[1;32m    134\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43moriginal_method\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:400\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, dataset\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Output column Geography_index already exists.",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Output column Geography_index already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tạo pipeline để xây dựng mô hình từ bước chuẩn hóa dữ liệu đến chọn mô hình học máy\n",
    "#Tạo pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler] + [lr])\n",
    "#Huấn luyện mô hình từ dữ liệu huấn luyện\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c53f7c-d4f6-4dd5-9d2a-faea026aa5b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Lưu mô hình xây dựng bao gồm dữ liệu đã huấn luyệ\n",
    "model.save(\"dbfs:/FileStore/shared_uploads/danghia2002@gmail.com/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc7e6e9-84d4-4742-b760-089ba900350d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2639336321475895>:4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PipelineModel\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#Load mô hình đã lưu\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m saved_model \u001B[38;5;241m=\u001B[39m PipelineModel\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/shared_uploads/danghia2002@gmail.com/model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#Dự đoán lại cho dữ liệu test\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m predictions \u001B[38;5;241m=\u001B[39m saved_model\u001B[38;5;241m.\u001B[39mtransform(test_data)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/util.py:447\u001B[0m, in \u001B[0;36mMLReadable.load\u001B[0;34m(cls, path)\u001B[0m\n",
       "\u001B[1;32m    444\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n",
       "\u001B[1;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RL:\n",
       "\u001B[1;32m    446\u001B[0m     \u001B[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 447\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:284\u001B[0m, in \u001B[0;36mPipelineModelReader.load\u001B[0;34m(self, path)\u001B[0m\n",
       "\u001B[1;32m    282\u001B[0m metadata \u001B[38;5;241m=\u001B[39m DefaultParamsReader\u001B[38;5;241m.\u001B[39mloadMetadata(path, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msc)\n",
       "\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlanguage\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparamMap\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mor\u001B[39;00m metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparamMap\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlanguage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mJavaMLReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mType\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mJavaMLReadable[PipelineModel]\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    286\u001B[0m     uid, stages \u001B[38;5;241m=\u001B[39m PipelineSharedReadWrite\u001B[38;5;241m.\u001B[39mload(metadata, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msc, path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/util.py:396\u001B[0m, in \u001B[0;36mJavaMLReader.load\u001B[0;34m(self, path)\u001B[0m\n",
       "\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n",
       "\u001B[1;32m    395\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath should be a string, got type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(path))\n",
       "\u001B[0;32m--> 396\u001B[0m java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_from_java\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n",
       "\u001B[1;32m    399\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz\n",
       "\u001B[1;32m    400\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.LogisticRegressionModel"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-2639336321475895>:4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PipelineModel\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#Load mô hình đã lưu\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m saved_model \u001B[38;5;241m=\u001B[39m PipelineModel\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/shared_uploads/danghia2002@gmail.com/model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#Dự đoán lại cho dữ liệu test\u001B[39;00m\n\u001B[1;32m      6\u001B[0m predictions \u001B[38;5;241m=\u001B[39m saved_model\u001B[38;5;241m.\u001B[39mtransform(test_data)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/util.py:447\u001B[0m, in \u001B[0;36mMLReadable.load\u001B[0;34m(cls, path)\u001B[0m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\u001B[38;5;28mcls\u001B[39m, path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RL:\n\u001B[1;32m    446\u001B[0m     \u001B[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 447\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:284\u001B[0m, in \u001B[0;36mPipelineModelReader.load\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    282\u001B[0m metadata \u001B[38;5;241m=\u001B[39m DefaultParamsReader\u001B[38;5;241m.\u001B[39mloadMetadata(path, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msc)\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlanguage\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparamMap\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mor\u001B[39;00m metadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparamMap\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlanguage\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mJavaMLReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mType\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mJavaMLReadable[PipelineModel]\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcls\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    286\u001B[0m     uid, stages \u001B[38;5;241m=\u001B[39m PipelineSharedReadWrite\u001B[38;5;241m.\u001B[39mload(metadata, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msc, path)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/ml/util.py:396\u001B[0m, in \u001B[0;36mJavaMLReader.load\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    395\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath should be a string, got type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(path))\n\u001B[0;32m--> 396\u001B[0m java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_from_java\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m    399\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clazz\n\u001B[1;32m    400\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.LogisticRegressionModel",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.LogisticRegressionModel",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Mở lại mô hình đã lưu và dự đoán lại cho dữ liệu test\n",
    "from pyspark.ml import PipelineModel\n",
    "#Load mô hình đã lưu\n",
    "saved_model = PipelineModel.load(\"dbfs:/FileStore/shared_uploads/danghia2002@gmail.com/model\")\n",
    "#Dự đoán lại cho dữ liệu test\n",
    "predictions = saved_model.transform(test_data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "baithuchanh7",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
